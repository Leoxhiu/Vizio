{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58a37f2",
   "metadata": {},
   "source": [
    "# Project setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7ce32",
   "metadata": {},
   "source": [
    "## Install and import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2769c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update pip -> pip install --upgrade pip\n",
    "# python 3.10 -> conda create --name viziotf python=3.10\n",
    "# tensorflow 2.10.0 -> pip install tensorflow==2.10\n",
    "# CUDA 11.2 -> https://developer.nvidia.com/cuda-11.2.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal\n",
    "# CuDNN 8.1 -> https://developer.nvidia.com/rdp/cudnn-archive\n",
    "# Protobuf -> pip install protobuf==3.20.1\n",
    "\n",
    "# check package\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee466b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import wget\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1eb8a",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebba265",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'vizio12' \n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('FYP', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('FYP','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('FYP','models'),\n",
    "    'PROTOC_PATH':os.path.join('FYP','protoc'),\n",
    "    'RESOURCES_PATH': os.path.join('FYP', 'workspace','resources'),\n",
    "    'IMAGES_PATH': os.path.join('FYP', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('FYP', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('FYP', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('FYP', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('FYP', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFLITE_PATH':os.path.join('FYP', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'RESULT_PATH': os.path.join('FYP', 'workspace','models',CUSTOM_MODEL_NAME, 'result'), \n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('FYP', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['RESOURCES_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c141e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        !mkdir {path}\n",
    "        print('New folder and files created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bc24e",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef803d",
   "metadata": {},
   "source": [
    "## Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE):\n",
    "    \n",
    "    # Get a list of all image files in the folder\n",
    "    image_files = [file for file in os.listdir(IMAGE_PATH) if file.lower().endswith('.jpg')]\n",
    "\n",
    "    # Randomly shuffle the image files\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Get the desired number of images for sampling\n",
    "    image_files_sampled = image_files[:SAMPLE_SIZE]\n",
    "\n",
    "    # Copy the sampled images to the destination directory\n",
    "    for image_file in image_files_sampled:\n",
    "        shutil.copy(os.path.join(IMAGE_PATH, image_file), os.path.join(OUTPUT_PATH, image_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e27b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/entrance_front'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/entrance_front'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)\n",
    "\n",
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/entrance_left'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/entrance_left'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)\n",
    "\n",
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/entrance_right'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/entrance_right'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)\n",
    "\n",
    "\n",
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/escalator_front'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/escalator_front'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)\n",
    "\n",
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/escalator_left'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/escalator_left'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)\n",
    "\n",
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/escalator_right'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/escalator_right'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)\n",
    "\n",
    "\n",
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/stair_front'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/stair_front'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)\n",
    "\n",
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/stair_left'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/stair_left'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)\n",
    "\n",
    "IMAGE_PATH = 'D:/# fyp_dataset/2 organized/stair_right'\n",
    "OUTPUT_PATH = 'D:/# fyp_dataset/final-sampling/stair_right'\n",
    "SAMPLE_SIZE = 1560\n",
    "random_sampling(IMAGE_PATH, OUTPUT_PATH, SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38487d5",
   "metadata": {},
   "source": [
    "## Image resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2aded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME):\n",
    "    \n",
    "    counter = 1\n",
    "    \n",
    "    # Get images from the path\n",
    "    image_files = os.listdir(IMAGE_PATH)\n",
    "    \n",
    "    for image in image_files:\n",
    "        \n",
    "        image_path = os.path.join(IMAGE_PATH, image)\n",
    "    \n",
    "        # Generate the final path with the new file name\n",
    "        new_file_name = f'{OUTPUT_NAME}_{counter}.jpg'\n",
    "        final_path = os.path.join(OUTPUT_PATH, new_file_name)\n",
    "        \n",
    "        # get the original image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Resize while maintaining aspect ratio using INTER_CUBIC interpolation\n",
    "        aspect_ratio = image.shape[1] / image.shape[0]\n",
    "        new_width = int(TARGET_SIZE[1] * aspect_ratio)\n",
    "        resized_image = cv2.resize(image, (new_width, TARGET_SIZE[1]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Create a canvas of the target size and place the resized image in the center\n",
    "        canvas = np.zeros((TARGET_SIZE[1], TARGET_SIZE[0], 3), dtype=np.uint8)\n",
    "        x_offset = (TARGET_SIZE[0] - new_width) // 2\n",
    "        canvas[:, x_offset:x_offset+new_width, :] = resized_image\n",
    "    \n",
    "        # Save the modified image to the specified output path\n",
    "        cv2.imwrite(final_path, canvas.astype(np.uint8))\n",
    "        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/entrance_front'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/entrance_front'\n",
    "# OUTPUT_NAME = 'entrance_front'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/entrance_left'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/entrance_left'\n",
    "# OUTPUT_NAME = 'entrance_left'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/entrance_right'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/entrance_right'\n",
    "# OUTPUT_NAME = 'entrance_right'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "\n",
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/escalator_front'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/escalator_front'\n",
    "# OUTPUT_NAME = 'escalator_front'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/escalator_left'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/escalator_left'\n",
    "# OUTPUT_NAME = 'escalator_left'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/escalator_right'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/escalator_right'\n",
    "# OUTPUT_NAME = 'escalator_right'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "\n",
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/stair_front'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/stair_front'\n",
    "# OUTPUT_NAME = 'stair_front'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/stair_left'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/stair_left'\n",
    "# OUTPUT_NAME = 'stair_left'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "# IMAGE_PATH = 'D:/# fyp_dataset/final-sampling/stair_right'\n",
    "# TARGET_SIZE = (320, 320)\n",
    "# OUTPUT_PATH = 'D:/# fyp_dataset/final-resized/stair_right'\n",
    "# OUTPUT_NAME = 'stair_right'\n",
    "# resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "IMAGE_PATH = 'D:\\\\# fyp_dataset\\\\references\\\\original\\\\mrt muzium negara\\\\entrance'\n",
    "TARGET_SIZE = (320, 320)\n",
    "OUTPUT_PATH = 'D:\\\\# fyp_dataset\\\\references\\\\original\\\\mrt muzium negara\\\\entrance'\n",
    "OUTPUT_NAME = 'reference'\n",
    "resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "IMAGE_PATH = 'D:\\\\# fyp_dataset\\\\references\\\\original\\\\mrt muzium negara\\\\escalator'\n",
    "TARGET_SIZE = (320, 320)\n",
    "OUTPUT_PATH = 'D:\\\\# fyp_dataset\\\\references\\\\original\\\\mrt muzium negara\\\\escalator'\n",
    "OUTPUT_NAME = 'reference'\n",
    "resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)\n",
    "\n",
    "IMAGE_PATH = 'D:\\\\# fyp_dataset\\\\references\\\\original\\\\mrt muzium negara\\\\stair'\n",
    "TARGET_SIZE = (320, 320)\n",
    "OUTPUT_PATH = 'D:\\\\# fyp_dataset\\\\references\\\\original\\\\mrt muzium negara\\\\stair'\n",
    "OUTPUT_NAME = 'reference'\n",
    "resize_image(IMAGE_PATH, TARGET_SIZE, OUTPUT_PATH, OUTPUT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71292cc",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fe0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(base_path, image_src, annotation_src, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "\n",
    "    # Source folders\n",
    "    image_folder = os.path.join(base_path, image_src)\n",
    "    annotation_folder = os.path.join(base_path, annotation_src)\n",
    "    \n",
    "    # Destination folders\n",
    "    folders = {\n",
    "        'train': {\n",
    "            'images': os.path.join(base_path, 'train', 'images'),\n",
    "            'annotations': os.path.join(base_path, 'train', 'annotations')\n",
    "        },\n",
    "        'validate': {\n",
    "            'images': os.path.join(base_path, 'validate', 'images'),\n",
    "            'annotations': os.path.join(base_path, 'validate', 'annotations')\n",
    "        },\n",
    "        'test': {\n",
    "            'images': os.path.join(base_path, 'test', 'images'),\n",
    "            'annotations': os.path.join(base_path, 'test', 'annotations')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    image_extensions = ['.jpg']\n",
    "    annotation_extensions = ['.xml']\n",
    "    \n",
    "    imgs_list = [filename for filename in os.listdir(image_folder) if os.path.splitext(filename)[-1] in image_extensions]\n",
    "    annotations_list = [os.path.splitext(img)[0] + ext for img in imgs_list for ext in annotation_extensions]\n",
    "\n",
    "    random.seed(42)\n",
    "    combined = list(zip(imgs_list, annotations_list))\n",
    "    random.shuffle(combined)\n",
    "    imgs_list, annotations_list = zip(*combined)\n",
    "\n",
    "    sizes = {\n",
    "        'train': int(len(imgs_list) * train_ratio),\n",
    "        'validate': int(len(imgs_list) * val_ratio),\n",
    "        'test': int(len(imgs_list) * test_ratio)\n",
    "    }\n",
    "\n",
    "    # Create destination sub-folders if they don't exist\n",
    "    for split, paths in folders.items():\n",
    "        for kind, path in paths.items():\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "    # Copy image and annotation files to destination sub-folders\n",
    "    for i, (img, annotation) in enumerate(zip(imgs_list, annotations_list)):\n",
    "        if i < sizes['train']:\n",
    "            split = 'train'\n",
    "        elif i < sizes['train'] + sizes['validate']:\n",
    "            split = 'validate'\n",
    "        else:\n",
    "            split = 'test'\n",
    "        \n",
    "        shutil.copy(os.path.join(image_folder, img), os.path.join(folders[split]['images'], img))\n",
    "        shutil.copy(os.path.join(annotation_folder, annotation), os.path.join(folders[split]['annotations'], annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb702e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'D:/# fyp_dataset/balanced'\n",
    "split_dataset(base_path, 'entrance_front', 'entrance_front_annotations')\n",
    "split_dataset(base_path, 'entrance_left', 'entrance_left_annotations')\n",
    "split_dataset(base_path, 'entrance_right', 'entrance_right_annotations')\n",
    "split_dataset(base_path, 'escalator_front', 'escalator_front_annotations')\n",
    "split_dataset(base_path, 'escalator_left', 'escalator_left_annotations')\n",
    "split_dataset(base_path, 'escalator_right', 'escalator_right_annotations')\n",
    "split_dataset(base_path, 'stair_front', 'stair_front_annotations')\n",
    "split_dataset(base_path, 'stair_left', 'stair_left_annotations')\n",
    "split_dataset(base_path, 'stair_right', 'stair_right_annotations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18781ce3",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    return cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b11f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(output_path, image):\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    bboxes = []\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        x_min = int(bbox.find('xmin').text)\n",
    "        y_min = int(bbox.find('ymin').text)\n",
    "        x_max = int(bbox.find('xmax').text)\n",
    "        y_max = int(bbox.find('ymax').text)\n",
    "        \n",
    "        bboxes.append((x_min, y_min, x_max, y_max))\n",
    "        labels.append(label)\n",
    "\n",
    "    return bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b426a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annotation(output_path, bboxes, labels, template_xml, new_image_name):\n",
    "    tree = ET.parse(template_xml)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Update filename in the XML\n",
    "    filename_xml = root.find('filename')\n",
    "    if filename_xml is not None:\n",
    "        filename_xml.text = new_image_name\n",
    "    \n",
    "    # Remove existing objects in the template\n",
    "    for obj in root.findall('object'):\n",
    "        root.remove(obj)\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        # Create a new object element and populate it\n",
    "        obj = ET.SubElement(root, 'object')\n",
    "        ET.SubElement(obj, 'name').text = label\n",
    "        ET.SubElement(obj, 'pose').text = 'Unspecified'\n",
    "        ET.SubElement(obj, 'truncated').text = '0'\n",
    "        ET.SubElement(obj, 'difficult').text = '0'\n",
    "        \n",
    "        bbox_xml = ET.SubElement(obj, 'bndbox')\n",
    "        ET.SubElement(bbox_xml, 'xmin').text = str(int(bbox[0]))\n",
    "        ET.SubElement(bbox_xml, 'ymin').text = str(int(bbox[1]))\n",
    "        ET.SubElement(bbox_xml, 'xmax').text = str(int(bbox[2]))\n",
    "        ET.SubElement(bbox_xml, 'ymax').text = str(int(bbox[3]))\n",
    "\n",
    "    tree.write(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_and_augmentation(IMAGE_PATH, ANNOTATION_PATH, OUTPUT_IMAGE_PATH, OUTPUT_ANNOTATION_PATH, SAMPLE_RATIO):\n",
    "    \n",
    "    image_height = 320\n",
    "    image_width = 320\n",
    "    NUM_AUGMENTATIONS_PER_IMAGE = 3\n",
    "\n",
    "    augmentation_pipeline = A.Compose([\n",
    "        \n",
    "        # Hue and Saturation\n",
    "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "\n",
    "        # Contrast and Brightness\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "\n",
    "        # Random zooming with limited reduction\n",
    "        A.RandomSizedCrop(min_max_height=(int(0.8*image_height), image_height), height=image_height, width=image_width, p=0.5),\n",
    "\n",
    "        # Affine and Geometric transformations\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "\n",
    "        # Mild Perspective change\n",
    "        A.Perspective(scale=(0.02, 0.05), p=0.5),\n",
    "\n",
    "        # Blurs\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.GaussianBlur(p=0.2),\n",
    "\n",
    "        # Noise\n",
    "        A.ISONoise(p=0.5)\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "    \n",
    "    # Get a list of all image files in the folder\n",
    "    image_files = [file for file in os.listdir(IMAGE_PATH) if file.lower().endswith('.jpg')]\n",
    "\n",
    "    # Randomly shuffle the image files\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Determine the sample size based on the desired ratio\n",
    "    sample_size = int(len(image_files) * SAMPLE_RATIO)\n",
    "\n",
    "    # Get the desired number of images for sampling\n",
    "    image_files_sampled = image_files[:sample_size]\n",
    "\n",
    "    # For each sampled image, apply augmentation and save both image and annotation\n",
    "    for image_file in image_files_sampled:\n",
    "        \n",
    "        # Load the image\n",
    "        image = load_image(os.path.join(IMAGE_PATH, image_file))\n",
    "        \n",
    "        # Load the corresponding annotation\n",
    "        annotation_name = image_file.replace(\".jpg\", \".xml\")\n",
    "        bboxes, labels = load_annotation(os.path.join(ANNOTATION_PATH, annotation_name))\n",
    "        \n",
    "        for i in range(NUM_AUGMENTATIONS_PER_IMAGE):\n",
    "            # Apply the augmentation\n",
    "            transformed = augmentation_pipeline(image=image, bboxes=bboxes, labels=labels)\n",
    "            transformed_image = transformed[\"image\"]\n",
    "            transformed_bboxes = transformed[\"bboxes\"]\n",
    "            \n",
    "            # Save the augmented image with a new name\n",
    "            new_image_name = f\"aug_{i}_\" + image_file\n",
    "            save_image(os.path.join(OUTPUT_IMAGE_PATH, new_image_name), transformed_image)\n",
    "            \n",
    "            # Save the updated annotation with a corresponding name\n",
    "            new_annotation_name = new_image_name.replace(\".jpg\", \".xml\")\n",
    "            template_xml = os.path.join(ANNOTATION_PATH, annotation_name)\n",
    "            save_annotation(os.path.join(OUTPUT_ANNOTATION_PATH, new_annotation_name), transformed_bboxes, \n",
    "                            labels, template_xml, new_image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(paths['IMAGES_PATH'], 'train', 'images')\n",
    "ANNOTATION_PATH = os.path.join(paths['IMAGES_PATH'], 'train', 'annotations')\n",
    "OUTPUT_IMAGE_PATH = os.path.join(paths['IMAGES_PATH'], 'augmented', 'images')\n",
    "OUTPUT_ANNOTATION_PATH = os.path.join(paths['IMAGES_PATH'], 'augmented', 'annotations')\n",
    "SAMPLE_RATIO = 0.7\n",
    "\n",
    "random_sampling_and_augmentation(IMAGE_PATH, ANNOTATION_PATH, OUTPUT_IMAGE_PATH, OUTPUT_ANNOTATION_PATH, SAMPLE_RATIO)\n",
    "print('Data augmentation completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdb715",
   "metadata": {},
   "source": [
    "## Image dimension checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e627d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image_dimensions(IMAGE_PATH, TARGET_WIDTH, TARGET_HEIGHT):\n",
    "    \n",
    "    for image_file in os.listdir(IMAGE_PATH):\n",
    "        if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            with Image.open(os.path.join(IMAGE_PATH, image_file)) as img:\n",
    "                width, height = img.size\n",
    "                if width != TARGET_WIDTH or height != TARGET_HEIGHT:\n",
    "                    print(f\"Image {image_file} has dimensions {width}x{height} instead of {TARGET_WIDTH}x{TARGET_HEIGHT}\")\n",
    "                    return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(paths['IMAGES_PATH'], 'train', 'images')\n",
    "TARGET_WIDTH = 320\n",
    "TARGET_HEIGHT = 320\n",
    "\n",
    "if check_image_dimensions(IMAGE_PATH, TARGET_WIDTH, TARGET_HEIGHT):\n",
    "    print(\"All images have the desired dimensions!\")\n",
    "else:\n",
    "    print(\"Some images do not match the desired dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31091e64",
   "metadata": {},
   "source": [
    "## Images with corresponding annotations checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d45203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_annotations(IMAGE_PATH, ANNOTATION_PATH):\n",
    "   \n",
    "    missing_annotations = []\n",
    "\n",
    "    # Iterate over each image in the image folder\n",
    "    for image_file in os.listdir(IMAGE_PATH):\n",
    "        if image_file.lower().endswith(('.jpg')):\n",
    "            # Construct the expected annotation filename\n",
    "            expected_annotation = os.path.splitext(image_file)[0] + '.xml'\n",
    "            \n",
    "            # Check if the annotation exists\n",
    "            if not os.path.exists(os.path.join(ANNOTATION_PATH, expected_annotation)):\n",
    "                missing_annotations.append(image_file)\n",
    "\n",
    "    # If there are missing annotations, print them and return False\n",
    "    if missing_annotations:\n",
    "        print(\"The following images are missing annotations:\")\n",
    "        for image in missing_annotations:\n",
    "            print(image)\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = os.path.join(paths['IMAGES_PATH'], 'train', 'images')\n",
    "ANNOTATION_PATH = os.path.join(paths['IMAGES_PATH'], 'train', 'annotations')\n",
    "if check_annotations(IMAGE_PATH, ANNOTATION_PATH):\n",
    "    print(\"All images have corresponding annotations!\")\n",
    "else:\n",
    "    print(\"Some images are missing annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60ca31",
   "metadata": {},
   "source": [
    "## Files renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621aecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the images\n",
    "directory_path = 'D:/# fyp_dataset/final-labeled/stair_left'\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.startswith(\"stair_left\"):\n",
    "        # Extract the number and the extension\n",
    "        parts = filename.split(\"_\")\n",
    "        number_part = parts[2].split('.')[0]\n",
    "        extension = parts[2].split('.')[1]\n",
    "\n",
    "        # Create the new filename\n",
    "        new_filename = f\"stair_right_{number_part}.{extension}\"\n",
    "\n",
    "        # Rename the file\n",
    "        os.rename(os.path.join(directory_path, filename), os.path.join(directory_path, new_filename))\n",
    "        print(f\"Renamed {filename} to {new_filename}\")\n",
    "\n",
    "print(\"Renaming process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6f1be",
   "metadata": {},
   "source": [
    "## Convert XML to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_csv(base_path, data_type):\n",
    "    assert data_type in ['train', 'test', 'validate'], \"data_type should be either 'train', 'test', or 'validate'\"\n",
    "    \n",
    "    # Navigating to the annotations folder inside the data_type directory\n",
    "    path = os.path.join(base_path, data_type, 'annotations')\n",
    "    \n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "            \n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    csv_path = os.path.join(base_path, f'{data_type}.csv')\n",
    "    xml_df.to_csv(csv_path, index=None)\n",
    "    \n",
    "    print(f'Successfully converted xml in {data_type}/annotations folder to {data_type}.csv in the base path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = paths['IMAGES_PATH']\n",
    "#base_path = \"D:\\\\# fyp_dataset\\\\balanced\"\n",
    "for dtype in ['train', 'test', 'validate']:\n",
    "    xml_to_csv(base_path, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b3074",
   "metadata": {},
   "source": [
    "# CSV checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_value_counts(file_path, column_name):\n",
    "    \"\"\"Load CSV, and display unique value counts for a specific column.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    value_counts = df[column_name].value_counts()\n",
    "    \n",
    "    print(f\"File: {os.path.basename(file_path)}\")\n",
    "    for category, count in value_counts.items():\n",
    "        print(f\"{count} {category}\")\n",
    "    print(\"-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd33962",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    os.path.join(paths['RESOURCES_PATH'], 'train.csv'),\n",
    "    os.path.join(paths['RESOURCES_PATH'], 'validate.csv'),\n",
    "    os.path.join(paths['RESOURCES_PATH'], 'test.csv')\n",
    "]\n",
    "\n",
    "column_name = 'class'\n",
    "for csv_file in csv_files:\n",
    "    display_value_counts(csv_file, column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5890294",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f48af4",
   "metadata": {},
   "source": [
    "## Setup object detection api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e5ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup protobuf\n",
    "os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['PROTOC_PATH'], 'bin'))   \n",
    "!cd FYP/models/research && protoc object_detection/protos/*.proto --python_out=. && copy object_detection\\\\packages\\\\tf2\\\\setup.py setup.py && python setup.py build && python setup.py install\n",
    "!cd FYP/models/research/slim && pip install -e . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "# Verify Installation\n",
    "!python {VERIFICATION_SCRIPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d452c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf388f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(PRETRAINED_MODEL_URL)\n",
    "!move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "!cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1c21a",
   "metadata": {},
   "source": [
    "## Create labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [{'name':'entrance_front', 'id':1}, {'name':'entrance_left', 'id':2}, {'name':'entrance_right', 'id':3}, \n",
    "          {'name':'escalator_front', 'id':4}, {'name':'escalator_left', 'id':5}, {'name':'escalator_right', 'id':6}, \n",
    "          {'name':'stair_front', 'id':7}, {'name':'stair_left', 'id':8}, {'name':'stair_right', 'id':9}\n",
    "         ]\n",
    "\n",
    "with open(files['LABELMAP'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863aed3",
   "metadata": {},
   "source": [
    "## Create TF records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fae4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {files['TF_RECORD_SCRIPT']} --csv_input={os.path.join(paths['RESOURCES_PATH'], 'train.csv')} --image_dir={os.path.join(paths['IMAGES_PATH'], 'train', 'images')} --labels_path={files['LABELMAP']} --output_path={os.path.join(paths['RESOURCES_PATH'], 'train.record')} \n",
    "!python {files['TF_RECORD_SCRIPT']} --csv_input={os.path.join(paths['RESOURCES_PATH'], 'validate.csv')} --image_dir={os.path.join(paths['IMAGES_PATH'], 'validate', 'images')} --labels_path={files['LABELMAP']} --output_path={os.path.join(paths['RESOURCES_PATH'], 'validate.record')} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a225d",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec733c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=50000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9da494",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8498bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b32307",
   "metadata": {},
   "source": [
    "### Import model and necessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-51')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b3cd9",
   "metadata": {},
   "source": [
    "### Setup necessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8009b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_and_visualize_image(image_path, detect_fn, category_index, label_id_offset=1, max_boxes_to_draw=5, min_score_thresh=.7):\n",
    "    \"\"\"\n",
    "    Detects and visualizes objects in the image specified by the image path.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: Path to the image.\n",
    "    - detect_fn: Detection function.\n",
    "    - category_index: Index of categories.\n",
    "    - label_id_offset: Offset for label IDs.\n",
    "    - max_boxes_to_draw: Maximum number of boxes to draw.\n",
    "    - min_score_thresh: Minimum score threshold for visualizing a detection.\n",
    "    \n",
    "    Returns:\n",
    "    - width_in_pixels: Width of the detected object in pixels.\n",
    "    - image_np_with_detections: Image with visualized detections.\n",
    "    \"\"\"\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    image_np = np.array(img)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=max_boxes_to_draw,\n",
    "                min_score_thresh=min_score_thresh,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    image_height, image_width, _ = image_np_with_detections.shape\n",
    "    box = detections['detection_boxes'][0]\n",
    "    xmin = int(box[1] * image_width)\n",
    "    xmax = int(box[3] * image_width)\n",
    "    width_in_pixels = xmax - xmin\n",
    "\n",
    "    return width_in_pixels, image_np_with_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714e894",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    os.path.join(paths['IMAGES_PATH'], 'test', 'images', 'entrance_front_446.jpg'),\n",
    "    os.path.join(paths['IMAGES_PATH'], 'test', 'images', 'escalator_left_746.jpg'),\n",
    "    os.path.join(paths['IMAGES_PATH'], 'test', 'images', 'stair_right_624.jpg'),\n",
    "#     # MRT Muzium Negara\n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/entrance/entrance_front3.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/entrance/entrance_left.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/entrance/entrance_right.jpg',\n",
    "\n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/escalator/escalator_front.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/escalator/escalator_left.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/escalator/escalator_right.jpg',\n",
    "    \n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/stair/stair_front.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/stair/stair_left.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/mrt muzium negara/stair/stair_right.jpg',\n",
    "    \n",
    "#     # LRT KL Sentral\n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/entrance/entrance_front3.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/entrance/entrance_left2.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/entrance/entrance_right.jpg',\n",
    "    \n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/escalator/escalator_front.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/escalator/escalator_left.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/escalator/escalator_right.jpg',\n",
    "    \n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/stair/stair_front.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/stair/stair_left.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt kl sentral/stair/stair_right.jpg',\n",
    "    \n",
    "#     # LRT Putra Heights\n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/entrance/entrance_front.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/entrance/entrance_left.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/entrance/entrance_right.jpg',\n",
    "    \n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/escalator/escalator_front.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/escalator/escalator_left.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/escalator/escalator_right.jpg',    \n",
    "    \n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/stair/stair_front2.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/stair/stair_left.jpg',\n",
    "#     'D:/# fyp_dataset/references/resized/lrt putra heights/stair/stair_right.jpg'\n",
    "]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    width, detected_image = detect_and_visualize_image(image_path, detect_fn, category_index)\n",
    "    plt.imshow(cv2.cvtColor(detected_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Width in pixels: {width}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0a40a",
   "metadata": {},
   "source": [
    "# Save and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4736669",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'exporter_main_v2.py ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --input_type=image_tensor --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(FREEZE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['OUTPUT_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18725ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960aa6f9",
   "metadata": {},
   "source": [
    "# TFlite conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa76f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLITE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'export_tflite_graph_tf2.py ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7aab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(TFLITE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['TFLITE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951eec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ef69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN_TFLITE_PATH = os.path.join(paths['TFLITE_PATH'], 'saved_model')\n",
    "TFLITE_MODEL = os.path.join(paths['TFLITE_PATH'], 'saved_model', 'vizio.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030fbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"tflite_convert \\\n",
    "--saved_model_dir={} \\\n",
    "--output_file={} \\\n",
    "--input_shapes=1,320,320,3 \\\n",
    "--input_arrays=normalized_input_image_tensor \\\n",
    "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
    "--inference_type=FLOAT \\\n",
    "--allow_custom_ops\".format(FROZEN_TFLITE_PATH, TFLITE_MODEL, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36c151",
   "metadata": {},
   "source": [
    "# TFlite model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ebcb7",
   "metadata": {},
   "source": [
    "## Random image evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f93c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(TFLITE_MODEL)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cccd9ef",
   "metadata": {},
   "source": [
    "## Input image evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=os.path.join(paths['TFLITE_PATH'], 'saved_model', 'vizio.tflite'))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Examine which output is which\n",
    "for detail in output_details:\n",
    "    print(detail['name'], detail['shape'])\n",
    "print()    \n",
    "\n",
    "# Load the test image\n",
    "im = cv2.imread('D:/# fyp_dataset/references/resized/lrt putra heights/escalator/escalator_left.jpg')\n",
    "\n",
    "# Convert the image from BGR to RGB\n",
    "im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize the image to match the input shape\n",
    "input_shape = input_details[0]['shape']\n",
    "im_rgb = cv2.resize(im_rgb, (input_shape[1], input_shape[2]))  # e.g., 320x320\n",
    "\n",
    "# Normalize the image to [0,1]\n",
    "input_data = np.expand_dims(im_rgb, axis=0).astype(np.float32) / 255.0\n",
    "\n",
    "# Set the input tensor for the interpreter\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the results\n",
    "detection_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "detection_locations = interpreter.get_tensor(output_details[1]['index'])\n",
    "num_boxes = interpreter.get_tensor(output_details[2]['index'])\n",
    "detection_classes = interpreter.get_tensor(output_details[3]['index'])\n",
    "\n",
    "# Print the results\n",
    "print(\"==detection_scores==\")\n",
    "print(detection_scores)\n",
    "print(\"\\n==detection_locations==\")\n",
    "print(detection_locations)\n",
    "print(\"\\n==num_boxes==\")\n",
    "print(num_boxes)\n",
    "print(\"\\n==detection_classes==\")\n",
    "print(detection_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb98a5",
   "metadata": {},
   "source": [
    "# Focal length calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5078e223",
   "metadata": {},
   "source": [
    "## Actual width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c75cf",
   "metadata": {},
   "source": [
    "### MRT Muzium Negara\n",
    "entrance = 54\\\n",
    "escalator = 59\\\n",
    "stair = 86\n",
    "\n",
    "\n",
    "### LRT KL Sentral\n",
    "entrance = 64\\\n",
    "escalator = 54\\\n",
    "stair = 60\n",
    "\n",
    "\n",
    "### LRT Putra Heights\n",
    "entrance = 64\\\n",
    "escalator = 57\\\n",
    "stair = 71"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea1543",
   "metadata": {},
   "source": [
    "### Find average of actual width for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c428bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average actual width of entrance: 60.666666666666664\n",
      "Average actual width of escalator: 56.666666666666664\n",
      "Average actual width of stair: 72.33333333333333\n"
     ]
    }
   ],
   "source": [
    "avg_entrance_width = (54+64+64) / 3\n",
    "avg_escalator_width = (59+54+57) / 3\n",
    "avg_stair_width = (86+60+71) / 3\n",
    "\n",
    "print(f'Average actual width of entrance: {avg_entrance_width}')\n",
    "print(f'Average actual width of escalator: {avg_escalator_width}')\n",
    "print(f'Average actual width of stair: {avg_stair_width}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a9359",
   "metadata": {},
   "source": [
    "## Width In Pixels (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aceb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRT Muzium Negara\n",
    "MMM_entrance_front = 72\n",
    "MMM_entrance_left = 52\n",
    "MMM_entrance_right = 45\n",
    "\n",
    "MMM_escalator_front = 69\n",
    "MMM_escalator_left = 68\n",
    "MMM_escalator_right = 65\n",
    "\n",
    "MMM_stair_front = 80\n",
    "MMM_stair_left = 57\n",
    "MMM_stair_right = 77\n",
    "\n",
    "\n",
    "# LRT KL Sentral\n",
    "LKS_entrance_front = 48\n",
    "LKS_entrance_left = 41\n",
    "LKS_entrance_right = 37\n",
    "\n",
    "LKS_escalator_front = 55\n",
    "LKS_escalator_left = 51\n",
    "LKS_escalator_right = 54\n",
    "\n",
    "LKS_stair_front = 57\n",
    "LKS_stair_left = 60\n",
    "LKS_stair_right = 55\n",
    "\n",
    "\n",
    "# LRT Putra Heights\n",
    "LPH_entrance_front = 46\n",
    "LPH_entrance_left = 32\n",
    "LPH_entrance_right = 37\n",
    "\n",
    "\n",
    "LPH_escalator_front = 53\n",
    "LPH_escalator_left = 43\n",
    "LPH_escalator_right = 31\n",
    "\n",
    "LPH_stair_front = 64\n",
    "LPH_stair_left = 53\n",
    "LPH_stair_right = 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb9acc",
   "metadata": {},
   "source": [
    "### Find average of Width In Pixels (WIP) for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034d1fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WIP of entrance front: 55.333333333333336\n",
      "Average WIP of entrance left: 41.666666666666664\n",
      "Average WIP of entrance right: 39.666666666666664\n",
      "Average WIP of escalator front: 59.0\n",
      "Average WIP of escalator left: 54.0\n",
      "Average WIP of escalator right: 50.0\n",
      "Average WIP of stair front: 67.0\n",
      "Average WIP of stair left: 56.666666666666664\n",
      "Average WIP of stair right: 63.0\n"
     ]
    }
   ],
   "source": [
    "avg_wip_entrance_front = (MMM_entrance_front + LKS_entrance_front + LPH_entrance_front) / 3\n",
    "avg_wip_entrance_left = (MMM_entrance_left + LKS_entrance_left + LPH_entrance_left) / 3\n",
    "avg_wip_entrance_right = (MMM_entrance_right + LKS_entrance_right + LPH_entrance_right) / 3\n",
    "avg_wip_escalator_front = (MMM_escalator_front + LKS_escalator_front + LPH_escalator_front) / 3\n",
    "avg_wip_escalator_left = (MMM_escalator_left + LKS_escalator_left + LPH_escalator_left) / 3\n",
    "avg_wip_escalator_right = (MMM_escalator_right + LKS_escalator_right + LPH_escalator_right) / 3\n",
    "avg_wip_stair_front = (MMM_stair_front + LKS_stair_front + LPH_stair_front) / 3\n",
    "avg_wip_stair_left = (MMM_stair_left + LKS_stair_left + LPH_stair_left) / 3\n",
    "avg_wip_stair_right = (MMM_stair_right + LKS_stair_right + LPH_stair_right) / 3\n",
    "\n",
    "print(f'Average WIP of entrance front: {avg_wip_entrance_front}')\n",
    "print(f'Average WIP of entrance left: {avg_wip_entrance_left}')\n",
    "print(f'Average WIP of entrance right: {avg_wip_entrance_right}')\n",
    "print(f'Average WIP of escalator front: {avg_wip_escalator_front}')\n",
    "print(f'Average WIP of escalator left: {avg_wip_escalator_left}')\n",
    "print(f'Average WIP of escalator right: {avg_wip_escalator_right}')\n",
    "print(f'Average WIP of stair front: {avg_wip_stair_front}')\n",
    "print(f'Average WIP of stair left: {avg_wip_stair_left}')\n",
    "print(f'Average WIP of stair right: {avg_wip_stair_right}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d6be8",
   "metadata": {},
   "source": [
    "### Find average of side angles (left and right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6523e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WIP of entrance side: 40.666666666666664\n",
      "Average WIP of escalator side: 52.0\n",
      "Average WIP of stair side: 59.83333333333333\n"
     ]
    }
   ],
   "source": [
    "# Left and right should be the same, thus finding average may help the accuracy\n",
    "\n",
    "avg_wip_entrance_side = (avg_wip_entrance_left + avg_wip_entrance_right) / 2\n",
    "avg_wip_escalator_side = (avg_wip_escalator_left + avg_wip_escalator_right) / 2\n",
    "avg_wip_stair_side = (avg_wip_stair_left + avg_wip_stair_right) / 2\n",
    "\n",
    "print(f'Average WIP of entrance side: {avg_wip_entrance_side}')\n",
    "print(f'Average WIP of escalator side: {avg_wip_escalator_side}')\n",
    "print(f'Average WIP of stair side: {avg_wip_stair_side}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978bc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_length_finder (measured_distance, real_width, width_in_pixels):\n",
    "    focal_length = (width_in_pixels * measured_distance) / real_width\n",
    "\n",
    "    return focal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c9b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUAL_DISTANCE = 80.0 #inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edce5706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal entrance front: 72.96703296703298\n",
      "Focal entrance side: 53.62637362637362\n",
      "Focal escalator front: 83.29411764705883\n",
      "Focal escalator side: 73.41176470588236\n",
      "Focal stair front: 74.10138248847927\n",
      "Focal stair side: 66.17511520737327\n"
     ]
    }
   ],
   "source": [
    "# entrance\n",
    "focal_entrance_front = focal_length_finder(ACTUAL_DISTANCE, \n",
    "                                           avg_entrance_width, \n",
    "                                           avg_wip_entrance_front)\n",
    "\n",
    "focal_entrance_side = focal_length_finder(ACTUAL_DISTANCE, \n",
    "                                           avg_entrance_width, \n",
    "                                           avg_wip_entrance_side)\n",
    "\n",
    "# escalator\n",
    "focal_escalator_front = focal_length_finder(ACTUAL_DISTANCE, \n",
    "                                           avg_escalator_width, \n",
    "                                           avg_wip_escalator_front)\n",
    "\n",
    "focal_escalator_side = focal_length_finder(ACTUAL_DISTANCE, \n",
    "                                           avg_escalator_width, \n",
    "                                           avg_wip_escalator_side)\n",
    "\n",
    "#stair\n",
    "focal_stair_front = focal_length_finder(ACTUAL_DISTANCE, \n",
    "                                           avg_stair_width, \n",
    "                                           avg_wip_stair_front)\n",
    "\n",
    "focal_stair_side = focal_length_finder(ACTUAL_DISTANCE, \n",
    "                                           avg_stair_width, \n",
    "                                           avg_wip_stair_side)\n",
    "\n",
    "\n",
    "print(f'Focal entrance front: {focal_entrance_front}')\n",
    "print(f'Focal entrance side: {focal_entrance_side}')\n",
    "print(f'Focal escalator front: {focal_escalator_front}')\n",
    "print(f'Focal escalator side: {focal_escalator_side}')\n",
    "print(f'Focal stair front: {focal_stair_front}')\n",
    "print(f'Focal stair side: {focal_stair_side}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
